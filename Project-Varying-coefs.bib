@article{cai2003,
  title = {Local {{Linear Estimation}} for {{Time-Dependent Coefficients}} in {{Cox}}'s {{Regression Models}}},
  author = {Cai, Zongwu and Sun, Yanqing},
  year = {2003},
  month = mar,
  journal = {Scandinavian Journal of Statistics},
  volume = {30},
  number = {1},
  pages = {93--111},
  publisher = {John Wiley \& Sons, Ltd},
  issn = {0303-6898},
  doi = {10.1111/1467-9469.00320},
  urldate = {2024-12-09},
  abstract = {This article develops a local partial likelihood technique to estimate the time-dependent coefficients in Cox's regression model. The basic idea is a simple extension of the local linear fitting technique used in the scatterplot smoothing. The coefficients are estimated locally based on the partial likelihood in a window around each time point. Multiple time-dependent covariates are incorporated in the local partial likelihood procedure. The procedure is useful as a diagnostic tool and can be used in uncovering time-dependencies or departure from the proportional hazards model. The programming involved in the local partial likelihood estimation is relatively simple and it can be modified with few efforts from the existing programs for the proportional hazards model. The asymptotic properties of the resulting estimator are established and compared with those from the local constant fitting. A consistent estimator of the asymptotic variance is also proposed. The approach is illustrated by a real data set from the study of gastric cancer patients and a simulation study is also presented.},
  keywords = {asymptotics,censored data,local constant fitting,local linear fitting,local partial likelihood,proportional hazards model,time-dependent covariate}
}

@article{crowley1977,
  title = {Covariance {{Analysis}} of {{Heart Transplant Survival Data}}},
  author = {Crowley, John and Hu, Marie},
  year = {1977},
  journal = {Journal of the American Statistical Association},
  volume = {72},
  number = {357},
  eprint = {2286902},
  eprinttype = {jstor},
  pages = {27--36},
  publisher = {[American Statistical Association, Taylor \& Francis, Ltd.]},
  issn = {0162-1459},
  doi = {10.2307/2286902},
  urldate = {2024-12-04},
  abstract = {This paper presents a number of analyses to assess the effects of various covariates on the survival of patients in the Stanford Heart Transplantation Program. The data have been updated from previously published versions and include some additional covariates, such as measures of tissue typing. The methods used allow for simultaneous investigation of several covariates and provide estimates of the relative risk of transplantation as well as significance tests.}
}

@article{hastie1993,
  title = {Varying-{{Coefficient Models}}},
  author = {Hastie, Trevor and Tibshirani, Robert},
  year = {1993},
  month = jan,
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {55},
  number = {4},
  pages = {757--779},
  issn = {0035-9246},
  doi = {10.1111/j.2517-6161.1993.tb01939.x},
  urldate = {2024-11-25},
  abstract = {We explore a class of regression and generalized regression models in which the coefficients are allowed to vary as smooth functions of other variables. General algorithms are presented for estimating the models flexibly and some examples are given. This class of models ties together generalized additive models and dynamic generalized linear models into one common framework. When applied to the proportional hazards model for survival data, this approach provides a new way of modelling departures from the proportional hazards assumption.}
}

@article{marra2012,
  title = {Coverage {{Properties}} of {{Confidence Intervals}} for {{Generalized Additive Model Components}}},
  author = {Marra, Giampiero and Wood, Simon N.},
  year = {2012},
  journal = {Scandinavian Journal of Statistics},
  volume = {39},
  number = {1},
  pages = {53--74},
  issn = {1467-9469},
  doi = {10.1111/j.1467-9469.2011.00760.x},
  urldate = {2024-12-09},
  abstract = {Abstract. We study the coverage properties of Bayesian confidence intervals for the smooth component functions of generalized additive models (GAMs) represented using any penalized regression spline approach. The intervals are the usual generalization of the intervals first proposed by Wahba and Silverman in 1983 and 1985, respectively, to the GAM component context. We present simulation evidence showing these intervals have close to nominal `across-the-function' frequentist coverage probabilities, except when the truth is close to a straight line/plane function. We extend the argument introduced by Nychka in 1988 for univariate smoothing splines to explain these results. The theoretical argument suggests that close to nominal coverage probabilities can be achieved, provided that heavy oversmoothing is avoided, so that the bias is not too large a proportion of the sampling variability. The theoretical results allow us to derive alternative intervals from a purely frequentist point of view, and to explain the impact that the neglect of smoothing parameter variability has on confidence interval performance. They also suggest switching the target of inference for component-wise intervals away from smooth components in the space of the GAM identifiability constraints.},
  copyright = {{\copyright} 2012 Board of the Foundation of the Scandinavian Journal of Statistics},
  langid = {english},
  keywords = {Bayesian confidence interval,generalized additive model,penalized regression spline}
}

@misc{srtr,
  title = {Scientific {{Registry}} of {{Transplant Recipients}}},
  author = {The SRTR Database},
  urldate = {2024-12-09},
  howpublished = {https://www.srtr.org/about-the-data/the-srtr-database/}
}

@misc{therneau2024,
  title = {Survival: {{Survival Analysis}}},
  shorttitle = {Survival},
  author = {Therneau, Terry M. and maintainer {until 2009)}, Thomas Lumley (original S.-{$>$}R port {and} R. and Elizabeth, Atkinson and Cynthia, Crowson},
  year = {2024},
  month = jun,
  urldate = {2024-12-09},
  abstract = {Contains the core survival analysis routines, including definition of Surv objects, Kaplan-Meier and Aalen-Johansen (multi-state) curves, Cox models, and parametric accelerated failure time models.},
  copyright = {LGPL-2 {\textbar} LGPL-2.1 {\textbar} LGPL-3 [expanded from: LGPL ({$\geq$} 2)]},
  keywords = {ClinicalTrials,Econometrics,Survival}
}

@article{wood2011,
  title = {Fast {{Stable Restricted Maximum Likelihood}} and {{Marginal Likelihood Estimation}} of {{Semiparametric Generalized Linear Models}}},
  author = {Wood, Simon N.},
  year = {2011},
  month = jan,
  journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume = {73},
  number = {1},
  pages = {3--36},
  issn = {1369-7412},
  doi = {10.1111/j.1467-9868.2010.00749.x},
  urldate = {2024-12-08},
  abstract = {Recent work by Reiss and Ogden provides a theoretical basis for sometimes preferring restricted maximum likelihood (REML) to generalized cross-validation (GCV) for smoothing parameter selection in semiparametric regression. However, existing REML or marginal likelihood (ML) based methods for semiparametric generalized linear models (GLMs) use iterative REML or ML estimation of the smoothing parameters of working linear approximations to the GLM. Such indirect schemes need not converge and fail to do so in a non-negligible proportion of practical analyses. By contrast, very reliable prediction error criteria smoothing parameter selection methods are available, based on direct optimization of GCV, or related criteria, for the GLM itself. Since such methods directly optimize properly defined functions of the smoothing parameters, they have much more reliable convergence properties. The paper develops the first such method for REML or ML estimation of smoothing parameters. A Laplace approximation is used to obtain an approximate REML or ML for any GLM, which is suitable for efficient direct optimization. This REML or ML criterion requires that Newton--Raphson iteration, rather than Fisher scoring, be used for GLM fitting, and a computationally stable approach to this is proposed. The REML or ML criterion itself is optimized by a Newton method, with the derivatives required obtained by a mixture of implicit differentiation and direct methods. The method will cope with numerical rank deficiency in the fitted model and in fact provides a slight improvement in numerical robustness on the earlier method of Wood for prediction error criteria based smoothness selection. Simulation results suggest that the new REML and ML methods offer some improvement in mean-square error performance relative to GCV or Akaike's information criterion in most cases, without the small number of severe undersmoothing failures to which Akaike's information criterion and GCV are prone. This is achieved at the same computational cost as GCV or Akaike's information criterion. The new approach also eliminates the convergence failures of previous REML- or ML-based approaches for penalized GLMs and usually has lower computational cost than these alternatives. Example applications are presented in adaptive smoothing, scalar on function regression and generalized additive model selection.}
}
